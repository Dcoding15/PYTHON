Feature Selection: -
-----------------

How to drop constant features using variance threshold ?
import pandas as pd
from sklearn.feature_selection import VarianceThreshold
df = pd.DataFrame(
    {
        "A":[1,2,4,1,2,4],
        "B":[4,5,6,7,8,9],
        "C":[0,0,0,0,0,0],
        "D":[1,1,1,1,1,1],
    }
)
low_variance = VarianceThreshold(threshold=0)
low_variance.fit(df)
constant_feature_columns = [i for i in df.columns if i not in df.columns[low_variance.get_support()]]
df.drop(constant_feature_columns,axis=1)

==============================================================================================================

How to drop high correlated independent features using Pearson Correlation ?
import pandas as pd
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.DataFrame()
y = df.loc[""]
X = df.drop("dependent feature name",axis=1)
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=0)
correlation = X_train.corr()
sns.heatmap(correlation,annot=True,cmap=plt.cm.CMRmap_r)
plt.show()

threshold = 0
col_corr = set()  # Set of all the names of correlated columns
corr_matrix = df.corr()
for i in range(len(corr_matrix.columns)):
     for j in range(i):
          if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
            colname = corr_matrix.columns[i]  # getting the name of column
            col_corr.add(colname)

==============================================================================================================

How to select most highly dependent features for classification problems ?
# It estimate mutual information for a discreate target variable.
import pandas as pd
df = pd.read_csv("https://gist.githubusercontent.com/tijptjik/9408623/raw/b237fa5848349a14a14e5d4107dc7897c21951f5/wine.csv")
y = df["Wine"]
X = df.drop("Wine",axis=1)

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import mutual_info_classif, SelectKBest
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

mutual_classification_info = mutual_info_classif(X_train, y_train)
mutual_classification_info = pd.Series(mutual_classification_info)
mutual_classification_info.index = X_train.columns

mutual_classification_info.sort_values(ascending=False).plot.bar()

select_8_best = SelectKBest(mutual_info_classif, k=8)
select_8_best.fit(X_train, y_train)

X_train.columns[select_8_best.get_support()]

==============================================================================================================

How to select most highly dependent featues for regression problems ?
# It estimate mutual information for a continuous target variable.
import pandas as pd
df = pd.read_csv()
y = df[""]
X = df.drop(y,axis=1)

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import mutual_info_regression, SelectPercentile
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
mutual_regression = mutual_info_regression(X_train.fillna(0), y_train)
mutual_regression_info = pd.Series(mutual_regression_info)
mutual_regression_info.index = X_train.columns

mutual_regression_info.sort_values(asending=False).plot.bar()

select_percentile = SelectPercentile(mutual_info_regression, percentile=)
select_percentile.fit(X_train, y_train)

X_train.columns[select_percentile.get_support()]

==============================================================================================================

How to select most highly dependent features using Chi2 test ?
# Before appling Chi2 test we have to perform label encoding on each columns
