Feature Enginnering: -
-------------------
    1. The process of using domain knowledge to extract features from raw data.
    2. It is used to improve machine learning algorithm.
    3. Types of feature enginnering: -
        a. Feature Transformation - missing values impuation, handling categorical features, outlier detection, feature scaling
        b. Feature Construction
        c. Feature Selection
        d. Feature Extraction

Feature Scaling: -
---------------
    1. It is a technique to standardise the independent features present in the data in a fixed range.
            

Types of Encoding: -
-----------------
    1. Nominal Encoding (consider numerical data)
        a. One Hot Encoding
        b. One Hot Encoding with many categories
        c. Mean Encoding
        d. Count of Frequency Encoding
    2. Ordinal Encoding (consider ranking data)
        a. Label Encoding
        b. Target Guided Ordinal Encoding

Handling Missing Values In Categorical Variable: -
-----------------------------------------------
    1. Delete the rows
    2. Replace with the most frequent values
    3. Apply classifier algorithm to predict (if any row has blank column then it will consider as test dataset)
    4. Apply unsupervised ML

Types of Transformation: -
-----------------------
    1. Normalisation and Standardisation: -
            from sklearn.preprocessing import StandardScaler
            x' = ( x[i] - mean(x) ) / standard_deviation(x), Range: -1 to 1
        Normalisation: -
    2. Scaling to Minimum and Maximum value: -
            from sklean.preprocessing import MinMaxScaler
            x' = (x[i] - min(x)) / (max(x) - min(x)), Range: 0 to 1
    3. Scaling to Median and Qualities: -
            from sklearn.preprocessing import RobustScaler
            x' = (x - median(x)) / (75th_quantile - 25th_quantile)
    4. Guassian Transformation: -
            First check whether feature is gaussian or normal distribution using Q-Q plot.
    5. Logarithmic Transformation: -
            import numpy as np
            new_feature_column = np.log(old_feature_column)
    6. Reciprocal Transformation: -
            new_feature_column = 1/old_feature_column
    7. Square root Transformation: -
            new_feature_column = old_feature_column**(1/2)
    8. Exponential Transformation: -
            new_feature_column = old_feature_column**(1/1.2)
    9. Box Cox Transformation: -
            T(Y) = (Y exp(lambda) - 1)/lambda
            Y is response variable
            lambda is transform parameters which varies from -5 to 5

            from scipy.stats import stat
            new_feature_column,parameters = stat.boxcox(old_feature_column)
    10. Fare Transformation: -
            import numpy as np
            new_feature_column = np.log1p(old_feature_column)

from sklearn.model_selection import train_test_split
from sklean.preprocessing import [name_of_feature_transformation]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random=10)
scaler = [name_of_feature_transformation]()
x_train_scaled = scaler.fit_transform(X_train)
x_test_scaled = scaler.transform(X_test)
Note: -
    X is independent feature
    y is dependent feature