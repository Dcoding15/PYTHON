# Simple Linear Regression

Residual Error -> Distance between predicted point and actual point.
Best-fit line  -> Summation of all residual error, which ever is lower.

Aim: To find best-fit line 

Linear Regression Function: -
Regression Line Slope or best-fit line: -
y = m*x + c
            where, y -> predictated value or dependent variable
                   m -> slope or coefficient
                   x -> independent variable
                   c -> intercept at y
Hypothesis: -
theta_h(x) = theta_0 + theta_1*x_1 + ... +  theta_n*x_n
                                  where, theta_h(x)             -> hypothesis function
                                         theta_0                -> y-intercept
                                         theta_1, ... , theta_n -> slope of regression line
                                         x_1, ... , x_n         -> independent variables

Cost Function or Mean Squared Error: -
J(theta_0,theta_1) = 1/(2*i) summation(theta_h(x_i) - y_i)^2
                                                             where, m -> no. of datapoints
                                                                    i -> starts from 0 to m

Convergence Theorem: -
theta_j := theta_j - lamda(derivative of theta_j(J(theta_0,theta_1))
                                                                    where, theta_j -> theta object which is minima of gradient descent
                                                                           lamda   -> learning rate which is best at low value


Practical Implementation: -
import pandas as pd
import numpy as np
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
df = load_diabetes()
dataset = pd.DataFrame(df.data)
# Breaking into dependent variables and independent variables
X = dataset		# Dependent features
y = df.target	# Independent features

# Performing train test split
X_train, X_test, y_train, y_test = train_split(X, y, test_size=0.3, random_state=42)

# Standardising datasets
sclr = StandardScaler()
X_train = sclr.fit_transform(X_train)
X_test = sclr.transform(X_test)

# Model training
mdl = LinearRegression()
mdl.fit(X_train,y_train)
mdl_predict = mdl.predict(X_test)

# Cross validataion
from sklearn.model_selection import cross_val_score
mse = cross_val_score(mdl, X_train, X_test, scoring='ned_mean_squared_error', cv=5)
np.mean(mse)
#Note: The more less value of mean the better

# Visualising
import seaborn as sns
sns.displot(mdl_predicit - y_test, kind=kde)
